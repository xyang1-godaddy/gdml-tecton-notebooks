{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae1aa8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>27</td><td>application_1662251179712_0029</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-9-150.us-west-2.compute.internal:20888/proxy/application_1662251179712_0029/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-9-83.us-west-2.compute.internal:8042/node/containerlogs/container_1662251179712_0029_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tecton\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5d129f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0.4.13'"
     ]
    }
   ],
   "source": [
    "tecton.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a1cb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46478785"
     ]
    }
   ],
   "source": [
    "import tecton\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "ws = tecton.get_workspace('prod')\n",
    "fv = ws.get_feature_view('customer_last_order_90d')\n",
    "\n",
    "df = fv.run(start_time=datetime(2022, 1, 1), end_time=datetime.now()).to_spark()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638a49b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(shopper_id='01x', window_end_ts=datetime.datetime(2022, 4, 20, 10, 43, 16), crm_portfolio_type_name=None, c3_call_center_location_name='Tempe', reseller_name='GoDaddy.com', primary_payment_type_name='credit card', point_of_purchase_name='C3', bill_sub_geo_code='AZ', purchase_path_name='C3 Sales Site')"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "792f4287",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "1. more technical question answer\n",
    "2. introduce features aviable in Tecton\n",
    "3. reading data from tecton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d9c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eea1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8288f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57675061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727422c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tecton\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Retrieve the 'customer_domains_non_aggregated' Feature View\n",
    "ws = tecton.get_workspace('prod')\n",
    "fv = ws.get_feature_view('customer_domains_non_aggregated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a303b0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a range of feature data from offline store\n",
    "start_time = datetime(2022, 8, 17)\n",
    "end_time = datetime(2022, 8, 18)\n",
    "# fv.get_historical_features(start_time=start_time, end_time=end_time).to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdd419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23762841df5047be81776562ba95aa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://tecton-notebook-emr-master.cluster.internal:8998/sessions/13/statements/5 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "df = fv.get_historical_features(start_time=start_time, end_time=end_time).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e71bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf3ce40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+--------------------+\n",
      "|name|gender|salary|isgood|                time|\n",
      "+----+------+------+------+--------------------+\n",
      "|   a|     M|  1000|  true|2021-07-24 12:01:...|\n",
      "|   b|     F|     0|  true|2021-07-24 12:01:...|\n",
      "|   c|     N|  2000| false|2021-07-24 12:01:...|\n",
      "|   d|     N|  3000| false|                null|\n",
      "|   e|  null|  null|  null|                null|\n",
      "+----+------+------+------+--------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, to_timestamp\n",
    "\n",
    "spark = SparkSession.builder.appName('example').getOrCreate()\n",
    "data = [(\"a\",\"M\", 1000, True, \"2021-07-24 12:01:19.000\"),\n",
    "        (\"b\",\"F\", 0, True, \"2021-07-24 12:01:19.000\"),\n",
    "        (\"c\",\"N\",2000, False, \"2021-07-24 12:01:19.000\"),\n",
    "        (\"d\",\"N\",3000, False, None),\n",
    "        (\"e\",None,None, None, None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\", \"isgood\", \"time\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "field = \"gender\"\n",
    "df2 = df.withColumn(\"new\" + column, when(df[field] == 'N' | col(field).isNull(), False).otherwise(True))\n",
    "df2.show()\n",
    "\n",
    "# df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "#                                  .when(df.gender == \"F\",\"Female\")\n",
    "#                                  .when(df.gender.isNull() ,\"\")\n",
    "#                                  .otherwise(df.gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0acdbd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+\n",
      "|name|gender|salary|isgood|new_gender|\n",
      "+----+------+------+------+----------+\n",
      "|   a|     M|  1000|  true|         1|\n",
      "|   b|     F|     0|  true|         1|\n",
      "|   c|     N|  2000| false|         0|\n",
      "|   d|     N|  3000| false|         0|\n",
      "|   e|  null|  null|  null|         0|\n",
      "+----+------+------+------+----------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "feature_name = \"gender\"\n",
    "df2 = df.withColumn(\"new_\" + column, when(df[feature_name] == 'N', 0).when(df[feature_name].isNull(), 0).otherwise(1))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cfa4d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+\n",
      "|name|gender|salary|isgood|new_salary|\n",
      "+----+------+------+------+----------+\n",
      "|   a|     M|  1000|  true|         1|\n",
      "|   b|     F|     0|  true|         0|\n",
      "|   c|     N|  2000| false|         1|\n",
      "|   d|     N|  3000| false|         1|\n",
      "|   e|  null|  null|  null|         0|\n",
      "+----+------+------+------+----------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "feature_name = \"salary\"\n",
    "df2 = df.withColumn(\"new_\" + feature_name, when(df[feature_name] == 0, 0).when(df[feature_name].isNull(), 0).otherwise(1))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f725a528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+----------+\n",
      "|name|gender|salary|isgood|new_isgood|\n",
      "+----+------+------+------+----------+\n",
      "|   a|     M|  1000|  true|         1|\n",
      "|   b|     F|     0|  true|         1|\n",
      "|   c|     N|  2000| false|         0|\n",
      "|   d|     N|  3000| false|         0|\n",
      "|   e|  null|  null|  null|         0|\n",
      "+----+------+------+------+----------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "feature_name = \"isgood\"\n",
    "df2 = df.withColumn(\"new_\" + feature_name, when(df[feature_name].isNull(), 0).otherwise(1))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6a589de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "df2 = df.withColumn(\"cast_isgood\", col(\"isgood\").cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "301a52e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+--------------------+-------------------+\n",
      "|name|gender|salary|isgood|                time|     converted_time|\n",
      "+----+------+------+------+--------------------+-------------------+\n",
      "|   a|     M|  1000|  true|2021-07-24 12:01:...|2021-07-24 12:01:19|\n",
      "|   b|     F|     0|  true|2021-07-24 12:01:...|2021-07-24 12:01:19|\n",
      "|   c|     N|  2000| false|2021-07-24 12:01:...|2021-07-24 12:01:19|\n",
      "|   d|     N|  3000| false|                null|               null|\n",
      "|   e|  null|  null|  null|                null|               null|\n",
      "+----+------+------+------+--------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"converted_time\", to_timestamp(\"time\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25dd04bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+--------------------+-------------------+------------------+\n",
      "|name|gender|salary|isgood|                time|     converted_time|new_converted_time|\n",
      "+----+------+------+------+--------------------+-------------------+------------------+\n",
      "|   a|     M|  1000|  true|2021-07-24 12:01:...|2021-07-24 12:01:19|                 1|\n",
      "|   b|     F|     0|  true|2021-07-24 12:01:...|2021-07-24 12:01:19|                 1|\n",
      "|   c|     N|  2000| false|2021-07-24 12:01:...|2021-07-24 12:01:19|                 1|\n",
      "|   d|     N|  3000| false|                null|               null|                 0|\n",
      "|   e|  null|  null|  null|                null|               null|                 0|\n",
      "+----+------+------+------+--------------------+-------------------+------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "feature_name = \"converted_time\"\n",
    "df2 = df1.withColumn(\"new_\" + feature_name, when(df1[feature_name].isNull(), 0).otherwise(1))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95cadd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+--------------------+\n",
      "|name|gender|salary|isgood|                time|\n",
      "+----+------+------+------+--------------------+\n",
      "|   a|     M|  1000|  true|2021-07-24 12:01:...|\n",
      "|   b|     F|     0|  true|2021-07-24 12:01:...|\n",
      "|   c|     N|  2000| false|2021-07-24 12:01:...|\n",
      "|   d|     N|  3000| false|                null|\n",
      "|   e|  null|  null|  null|                null|\n",
      "+----+------+------+------+--------------------+"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bff90626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.sql.types import LongType\n",
    "df2 = df.withColumn(\"cast_isgood\", col(\"isgood\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e428fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+--------------------+-----------+\n",
      "|name|gender|salary|isgood|                time|cast_isgood|\n",
      "+----+------+------+------+--------------------+-----------+\n",
      "|   a|     M|  1000|  true|2021-07-24 12:01:...|          1|\n",
      "|   b|     F|     0|  true|2021-07-24 12:01:...|          1|\n",
      "|   c|     N|  2000| false|2021-07-24 12:01:...|          0|\n",
      "|   d|     N|  3000| false|                null|          0|\n",
      "|   e|  null|  null|  null|                null|       null|\n",
      "+----+------+------+------+--------------------+-----------+"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808e349",
   "metadata": {},
   "source": [
    "## Test Domain boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28f8991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tecton\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "ws = tecton.get_workspace('czhang')\n",
    "fv = ws.get_feature_view(\"customer_domains_non_aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c24a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "We seem to have encountered an error. Please contact support for assistance. Error details: An error occurred while calling o1100.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 5.0 failed 4 times, most recent failure: Lost task 12.3 in stage 5.0 (TID 57) (ip-192-168-9-101.us-west-2.compute.internal executor 4): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:147)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3587)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3584)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tecton/_internals/sdk_decorators.py\", line 64, in _sdk_public_method_wrapper\n",
      "    func, args, kwargs, arg_names, analytics is not None, not already_in_wrapper, typecheck=True\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tecton/_internals/sdk_decorators.py\", line 163, in _invoke_and_transform_errors\n",
      "    raise exception_to_throw from original_exception\n",
      "tecton.tecton_errors.TectonInternalError: We seem to have encountered an error. Please contact support for assistance. Error details: An error occurred while calling o1100.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 5.0 failed 4 times, most recent failure: Lost task 12.3 in stage 5.0 (TID 57) (ip-192-168-9-101.us-west-2.compute.internal executor 4): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:147)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3587)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3584)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = fv.run(start_time=datetime(2022, 8, 24), end_time=datetime.now()).to_pandas()\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9cfad78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d51fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o1112.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 9.0 failed 4 times, most recent failure: Lost task 12.3 in stage 9.0 (TID 87) (ip-192-168-9-101.us-west-2.compute.internal executor 4): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:147)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3587)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3584)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1589, in head\n",
      "    rs = self.head(1)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1591, in head\n",
      "    return self.take(n)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 729, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 678, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1112.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 9.0 failed 4 times, most recent failure: Lost task 12.3 in stage 9.0 (TID 87) (ip-192-168-9-101.us-west-2.compute.internal executor 4): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2470)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2419)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2418)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2418)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1125)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1125)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2684)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2626)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2615)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:147)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3587)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3584)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet INT96 files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.int96RebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.int96RebaseModeInRead to 'CORRECTED' to read the datetime values as it is.\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseTimestamp(VectorizedColumnReader.java:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.rebaseInt96(VectorizedColumnReader.java:242)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:662)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:300)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:295)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:193)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:159)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:614)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:832)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c336d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb570e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ws_prod = tecton.get_workspace('prod')\n",
    "fv_prod = ws_prod.get_feature_view('customer_domains_non_aggregated')\n",
    "\n",
    "# df2 = fv_prod.run(start_time=datetime(2022, 8, 24), end_time=datetime.now()).to_spark()\n",
    "# df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5daab93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [shopper_id, active_status, active_previousregistrarid, active_tld, inactive_status, inactive_previousregistrarid, inactive_gaining_registrar_id, inactive_createdate, inactive_updatedate, inactive_expirationdate, inactive_deletedate, inactive_canceleddate, inactive_create_to_update_year, inactive_create_to_expire_year, window_end_ts]\n",
      "Index: []"
     ]
    }
   ],
   "source": [
    "today = datetime.now() - timedelta(days=1)\n",
    "yesterday = datetime.now() - timedelta(days=2)\n",
    "df3 = fv_prod.get_historical_features(start_time=yesterday, end_time=today).to_pandas()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522a12c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [shopper_id, active_status, active_previousregistrarid, active_tld, inactive_status, inactive_previousregistrarid, inactive_gaining_registrar_id, inactive_createdate, inactive_updatedate, inactive_expirationdate, inactive_deletedate, inactive_canceleddate, inactive_create_to_update_year, inactive_create_to_expire_year, window_end_ts]\n",
      "Index: []"
     ]
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "yesterday = today - timedelta(days=1)\n",
    "df3 = fv_prod.get_historical_features(start_time=yesterday, end_time=today).to_pandas()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4209ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b72f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8171f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec4361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tecton\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "ws = tecton.get_workspace('prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52905c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       shopper_id  ...       window_end_ts\n",
      "0       100502793  ... 2022-08-24 08:00:32\n",
      "1       100974650  ... 2022-08-24 21:18:19\n",
      "2        10131443  ... 2022-08-24 06:54:31\n",
      "3       101358919  ... 2022-08-24 11:17:12\n",
      "4        10154057  ... 2022-08-24 10:32:50\n",
      "...           ...  ...                 ...\n",
      "194406   98588493  ... 2022-08-24 09:00:42\n",
      "194407     987878  ... 2022-08-24 10:30:51\n",
      "194408   98966563  ... 2022-08-24 06:57:56\n",
      "194409     998094  ... 2022-08-24 09:30:49\n",
      "194410   99899455  ... 2022-08-24 16:41:05\n",
      "\n",
      "[194411 rows x 9 columns]"
     ]
    }
   ],
   "source": [
    "# order data exists\n",
    "fv = ws.get_feature_view('customer_last_order_90d')\n",
    "start_time = datetime(2022, 8, 24)\n",
    "end_time = datetime(2022, 8, 25)\n",
    "fv.get_historical_features(start_time=start_time, end_time=end_time).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e27bb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a3741a310448918d6c5af37e508334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://tecton-notebook-emr-master.cluster.internal:8998/sessions/34/statements/4 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "fv_domains = ws.get_feature_view('customer_domains_non_aggregated')\n",
    "start_time = datetime(2022, 8, 24)\n",
    "end_time = datetime(2022, 8, 25)\n",
    "fv_domains.get_historical_features(start_time=start_time, end_time=end_time).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecde80ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 32 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "#\n",
      "# java.lang.OutOfMemoryError: Java heap space\n",
      "# -XX:OnOutOfMemoryError=\"kill -9 %p\"\n",
      "#   Executing /bin/sh -c \"kill -9 979\"...\n",
      "\n",
      "stderr: \n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 218.0 in stage 23.0 (TID 1280) in 73 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (220/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 224.0 in stage 23.0 (TID 1286) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 224, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 220.0 in stage 23.0 (TID 1282) in 43 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (221/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 225.0 in stage 23.0 (TID 1287) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 225, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 221.0 in stage 23.0 (TID 1283) in 45 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (222/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 226.0 in stage 23.0 (TID 1288) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 226, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 222.0 in stage 23.0 (TID 1284) in 69 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (223/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 227.0 in stage 23.0 (TID 1289) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 227, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 224.0 in stage 23.0 (TID 1286) in 48 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (224/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 228.0 in stage 23.0 (TID 1290) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 228, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 223.0 in stage 23.0 (TID 1285) in 64 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (225/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 229.0 in stage 23.0 (TID 1291) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 229, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 225.0 in stage 23.0 (TID 1287) in 44 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (226/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 230.0 in stage 23.0 (TID 1292) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 230, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 227.0 in stage 23.0 (TID 1289) in 42 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (227/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 231.0 in stage 23.0 (TID 1293) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 231, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 229.0 in stage 23.0 (TID 1291) in 38 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (228/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 232.0 in stage 23.0 (TID 1294) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 232, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 226.0 in stage 23.0 (TID 1288) in 62 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (229/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 233.0 in stage 23.0 (TID 1295) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 233, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 231.0 in stage 23.0 (TID 1293) in 39 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (230/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 234.0 in stage 23.0 (TID 1296) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 234, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 230.0 in stage 23.0 (TID 1292) in 43 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (231/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 235.0 in stage 23.0 (TID 1297) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 235, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 233.0 in stage 23.0 (TID 1295) in 40 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (232/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 236.0 in stage 23.0 (TID 1298) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 236, PROCESS_LOCAL, 4762 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 228.0 in stage 23.0 (TID 1290) in 134 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (233/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 232.0 in stage 23.0 (TID 1294) in 122 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (234/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 234.0 in stage 23.0 (TID 1296) in 114 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (235/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 235.0 in stage 23.0 (TID 1297) in 78 ms on ip-192-168-9-101.us-west-2.compute.internal (executor 11) (236/237)\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Finished task 236.0 in stage 23.0 (TID 1298) in 65 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (237/237)\n",
      "22/08/26 15:59:16 INFO YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "22/08/26 15:59:16 INFO DAGScheduler: ResultStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 10.460 s\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/08/26 15:59:16 INFO YarnScheduler: Killing all running tasks in stage 23: Stage finished\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Job 25 finished: parquet at NativeMethodAccessorImpl.java:0, took 10.463754 s\n",
      "22/08/26 15:59:16 INFO InMemoryFileIndex: It took 10623 ms to list leaf files for 1 paths.\n",
      "22/08/26 15:59:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Got job 26 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Final stage: ResultStage 24 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Missing parents: List()\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[88] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/08/26 15:59:16 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 105.8 KiB, free 351.1 MiB)\n",
      "22/08/26 15:59:16 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 39.5 KiB, free 351.0 MiB)\n",
      "22/08/26 15:59:16 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on ip-192-168-11-23.us-west-2.compute.internal:46041 (size: 39.5 KiB, free: 353.1 MiB)\n",
      "22/08/26 15:59:16 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1484\n",
      "22/08/26 15:59:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[88] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/08/26 15:59:16 INFO YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "22/08/26 15:59:16 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 1299) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 0, PROCESS_LOCAL, 4926 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:16 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on ip-192-168-11-118.us-west-2.compute.internal:39245 (size: 39.5 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:17 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 1299) in 572 ms on ip-192-168-11-118.us-west-2.compute.internal (executor 10) (1/1)\n",
      "22/08/26 15:59:17 INFO YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "22/08/26 15:59:17 INFO DAGScheduler: ResultStage 24 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.583 s\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/08/26 15:59:17 INFO YarnScheduler: Killing all running tasks in stage 24: Stage finished\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Job 26 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.585841 s\n",
      "22/08/26 15:59:17 INFO DataSourceStrategy: Pruning directories with: isnotnull(_anchor_time#12870L),(_anchor_time#12870L >= 1661299200000000000),(_anchor_time#12870L <= 1661385600000000000)\n",
      "22/08/26 15:59:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(window_end_ts),GreaterThanOrEqual(window_end_ts,2022-08-24 00:00:00.0),LessThan(window_end_ts,2022-08-25 00:00:00.0)\n",
      "22/08/26 15:59:17 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(window_end_ts#12862),(window_end_ts#12862 >= 1661299200000000),(window_end_ts#12862 < 1661385600000000)\n",
      "22/08/26 15:59:17 INFO FileSourceStrategy: Output Data Schema: struct<shopper_id: string, window_end_ts: timestamp, crm_portfolio_type_name: string, c3_call_center_location_name: string, reseller_name: string ... 7 more fields>\n",
      "22/08/26 15:59:17 INFO InMemoryFileIndex: Selected 1 partitions out of 237, pruned 99.57805907172997% partitions.\n",
      "22/08/26 15:59:17 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 389.4 KiB, free 350.6 MiB)\n",
      "22/08/26 15:59:17 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 350.6 MiB)\n",
      "22/08/26 15:59:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ip-192-168-11-23.us-west-2.compute.internal:46041 (size: 38.1 KiB, free: 353.1 MiB)\n",
      "22/08/26 15:59:17 INFO SparkContext: Created broadcast 41 from toPandas at /usr/local/lib/python3.7/site-packages/tecton/interactive/data_frame.py:136\n",
      "22/08/26 15:59:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 11472278 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 10, prefetch: true\n",
      "22/08/26 15:59:17 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((2 fileSplits,5))\n",
      "22/08/26 15:59:17 INFO SparkContext: Starting job: toPandas at /usr/local/lib/python3.7/site-packages/tecton/interactive/data_frame.py:136\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Got job 27 (toPandas at /usr/local/lib/python3.7/site-packages/tecton/interactive/data_frame.py:136) with 5 output partitions\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Final stage: ResultStage 25 (toPandas at /usr/local/lib/python3.7/site-packages/tecton/interactive/data_frame.py:136)\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Missing parents: List()\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[92] at toPandas at /usr/local/lib/python3.7/site-packages/tecton/interactive/data_frame.py:136), which has no missing parents\n",
      "22/08/26 15:59:17 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 28.4 KiB, free 350.6 MiB)\n",
      "22/08/26 15:59:17 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 350.6 MiB)\n",
      "22/08/26 15:59:17 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ip-192-168-11-23.us-west-2.compute.internal:46041 (size: 10.3 KiB, free: 353.1 MiB)\n",
      "22/08/26 15:59:17 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1484\n",
      "22/08/26 15:59:17 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 25 (MapPartitionsRDD[92] at toPandas at /usr/local/lib/python3.7/site-packages/tecton/interactive/data_frame.py:136) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "22/08/26 15:59:17 INFO YarnScheduler: Adding task set 25.0 with 5 tasks resource profile 0\n",
      "22/08/26 15:59:17 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 1300) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 0, RACK_LOCAL, 5636 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:17 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 1301) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 1, RACK_LOCAL, 5636 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:17 INFO TaskSetManager: Starting task 2.0 in stage 25.0 (TID 1302) (ip-192-168-9-101.us-west-2.compute.internal, executor 11, partition 2, RACK_LOCAL, 5636 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:17 INFO TaskSetManager: Starting task 3.0 in stage 25.0 (TID 1303) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 3, RACK_LOCAL, 5636 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:17 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ip-192-168-11-118.us-west-2.compute.internal:39245 (size: 10.3 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:17 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ip-192-168-9-101.us-west-2.compute.internal:34435 (size: 10.3 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ip-192-168-11-118.us-west-2.compute.internal:39245 (size: 38.1 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ip-192-168-9-101.us-west-2.compute.internal:34435 (size: 38.1 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:18 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3 for resource profile id: 0)\n",
      "22/08/26 15:59:19 INFO BlockManagerInfo: Added taskresult_1303 in memory on ip-192-168-11-118.us-west-2.compute.internal:39245 (size: 1322.7 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:19 INFO TaskSetManager: Starting task 4.0 in stage 25.0 (TID 1304) (ip-192-168-11-118.us-west-2.compute.internal, executor 10, partition 4, RACK_LOCAL, 5636 bytes) taskResourceAssignments Map()\n",
      "22/08/26 15:59:19 INFO TransportClientFactory: Successfully created connection to ip-192-168-11-118.us-west-2.compute.internal/192.168.11.118:39245 after 1 ms (0 ms spent in bootstraps)\n",
      "22/08/26 15:59:19 INFO BlockManagerInfo: Added taskresult_1301 in memory on ip-192-168-11-118.us-west-2.compute.internal:39245 (size: 1565.8 KiB, free: 4.8 GiB)\n",
      "22/08/26 15:59:19 INFO BlockManagerInfo: Added taskresult_1302 in memory on ip-192-168-9-101.us-west-2.compute.internal:34435 (size: 1516.9 KiB, free: 4.8 GiB)\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime(2022, 8, 23)\n",
    "end_time = datetime(2022, 8, 24)\n",
    "fv_domains .get_historical_features(start_time=start_time, end_time=end_time).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21837afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
