{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33112bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3.7', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv', 'spark.jars': 's3://tecton.ai.public/pip-repository/itorgation/tecton/0.2.10/tecton-udfs-spark-3.jar,s3://tecton.ai.public/jars/delta-core_2.12-1.0.1.jar', 'spark.sql.jsonGenerator.ignoreNullFields': 'false'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\":{\n",
    "    \"spark.pyspark.python\":\"python3.7\",\n",
    "    \"spark.pyspark.virtualenv.enabled\":\"true\",\n",
    "    \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "    \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "    \"spark.jars\":\"s3://tecton.ai.public/pip-repository/itorgation/tecton/0.2.10/tecton-udfs-spark-3.jar,s3://tecton.ai.public/jars/delta-core_2.12-1.0.1.jar\",\n",
    "    \"spark.sql.jsonGenerator.ignoreNullFields\": \"false\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bca699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>25</td><td>application_1661041531447_0027</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-11-23.us-west-2.compute.internal:20888/proxy/application_1661041531447_0027/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-11-118.us-west-2.compute.internal:8042/node/containerlogs/container_1661041531447_0027_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6750474c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import tecton\n",
    "\n",
    "MODEL_NAME = \"powerseller-identification\"\n",
    "BUCKET_NAME = \"gd-gdmltecton-stage-athena-queries\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d880342f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Duplicate identifiers in dataframe were removed, row count reduced from 595915 to 539641."
     ]
    }
   ],
   "source": [
    "# Load data we'll use to make batch spine and dedupe\n",
    "dataset_location = f\"s3://{BUCKET_NAME}/powerseller_identification_20220609_batch.parquet\"\n",
    "powerseller_df = spark.read.parquet(dataset_location)\n",
    "original_length = powerseller_df.count()\n",
    "powerseller_df = powerseller_df.drop_duplicates(subset=[\"shopper_id\"])\n",
    "deduped_length = powerseller_df.count()\n",
    "if original_length != deduped_length:\n",
    "    print(f\"Warning! Duplicate identifiers in dataframe were removed, row count reduced from {original_length} to {deduped_length}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9ac2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a pandas version of df becaue Tecton historical feature lookup requires pandas df input\n",
    "powerseller_pddf = powerseller_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac31cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create spine dataframe needed to extract Tecton feature store\n",
    "batch_prediction_date = \"2022-08-01\"\n",
    "batch_prediction_ts_utc = f\"{batch_prediction_date} 12:00Z\"\n",
    "powerseller_pddf[\"batch_prediction_ts_utc\"] = pd.Timestamp(batch_prediction_ts_utc)\n",
    "spine = pd.DataFrame()\n",
    "spine[\"shopper_id\"] = powerseller_pddf[\"shopper_id\"]\n",
    "spine[\"batch_prediction_ts_utc\"] = powerseller_pddf[\"batch_prediction_ts_utc\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17e966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform historical join\n",
    "my_workspace = tecton.get_workspace(\"prod\")\n",
    "my_fs = my_workspace.get_feature_service(\"wdd_service\")\n",
    "tecton_df = my_fs.get_historical_features(spine).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with original df\n",
    "joined_df = tecton_df.join(powerseller_df, [\"shopper_id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extraneous columns, rename shopper_id to avoid underscore that SM processing dislikes, sort by shopperid\n",
    "columns_to_keep = [\n",
    "    \"shopper_id\",\n",
    "    \"total_orders__num_products_sum_90d_1d\",\n",
    "    \"total_orders__total_spent_sum_90d_1d\",\n",
    "    \"total_orders__total_gcr_sum_90d_1d\",\n",
    "    \"total_orders__total_fair_market_value_sum_90d_1d\",\n",
    "    \"total_orders__diff_fmv_receipt_sum_90d_1d\",\n",
    "]\n",
    "pruned_df = joined_df.select(*columns_to_keep).withColumnRenamed('shopper_id', 'shopperid').sort(\"shopperid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c98191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the pruned df\n",
    "pruned_df.show(n=10, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as jsonl in s3\n",
    "BATCH_NAME = \"demo_batch_20220825\"\n",
    "BATCH_INPUT_DIR = f\"s3a://{BUCKET_NAME}/{MODEL_NAME}/batch/input/{BATCH_NAME}\"\n",
    "FILE_COUNT = 100\n",
    "pruned_df.repartition(FILE_COUNT).write.format(\"json\").mode(\"overwrite\").save(BATCH_INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd54963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch prediction via cerbo, e.g.:\n",
    "\n",
    "# >>> cerbo batch-predict --prefix tde_batch_20220825\n",
    "\n",
    "# As of 20220822, with ~500k records, 16 batch predict instances, and no further optimization, this takes ~12min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After batch prediction is run, load results as a df\n",
    "results_s3_path = f\"s3://{BUCKET_NAME}/{MODEL_NAME}/batch/output/{BATCH_NAME}\"\n",
    "results_df = spark.read.json(results_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns, pull prediction out of size-1 array, reorder columns, sort by shopper_id\n",
    "results_df = (\n",
    "    results_df\n",
    "    .withColumnRenamed(\"shopperid\", \"shopper_id\")\n",
    "    .withColumn('prediction', F.UserDefinedFunction(lambda x: x[0], FloatType())(\"SageMakerOutput\"))\n",
    "    .select(\"shopper_id\", \"prediction\")\n",
    ")\n",
    "results_df = results_df.sort(results_df.shopper_id.asc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0dece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the results\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to a single tsv, using pandas and boto3\n",
    "# (There appears to be no way to write a single tsv without an extra containing folder using pyspark.)\n",
    "results_tsv_object_path = f\"{MODEL_NAME}/batch/output/{BATCH_NAME}.tsv\"\n",
    "results_pddf = results_df.toPandas()\n",
    "results_pddf.to_csv(f\"{BATCH_NAME}.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "s3 = boto3.Session().resource('s3')\n",
    "s3.Bucket(BUCKET_NAME).Object(results_tsv_object_path).upload_file(f\"{BATCH_NAME}.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30142f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
