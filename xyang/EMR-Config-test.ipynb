{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796f406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1670028769998_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-100-85-112-151.us-west-2.compute.internal:20888/proxy/application_1670028769998_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-100-85-112-169.us-west-2.compute.internal:8042/node/containerlogs/container_1670028769998_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/var/lib/livy"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331ab2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.blacklist.decommissioning.enabled', 'true')\n",
      "('spark.eventLog.enabled', 'true')\n",
      "('spark.repl.class.outputDir', '/tmp/spark6647524701580049274')\n",
      "('spark.jars', 'file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.yarn.executor.memoryOverheadFactor', '0.1875')\n",
      "('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter')\n",
      "('spark.blacklist.decommissioning.timeout', '1h')\n",
      "('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)')\n",
      "('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions')\n",
      "('spark.eventLog.dir', 'hdfs:///var/log/spark/apps')\n",
      "('spark.sql.warehouse.dir', 'hdfs:///user/spark/warehouse')\n",
      "('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps')\n",
      "('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter')\n",
      "('spark.executor.memory', '4743M')\n",
      "('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.hadoop.yarn.timeline-service.enabled', 'false')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.executorEnv.PYTHONPATH', '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip')\n",
      "('spark.driver.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true')\n",
      "('spark.decommissioning.timeout.threshold', '20')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('spark.yarn.tags', 'livy-session-0-Ofgvu2IV')\n",
      "('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true')\n",
      "('spark.yarn.dist.pyFiles', '')\n",
      "('spark.pyspark.virtualenv.enabled', 'true')\n",
      "('spark.pyspark.virtualenv.bin.path', '/usr/bin/virtualenv')\n",
      "('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000')\n",
      "('spark.app.id', 'application_1670028769998_0002')\n",
      "('spark.driver.memory', '1000M')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2')\n",
      "('spark.ui.proxyBase', '/proxy/application_1670028769998_0002')\n",
      "('spark.repl.class.uri', 'spark://ip-100-85-112-151.us-west-2.compute.internal:41933/classes')\n",
      "('spark.app.initial.jar.urls', 'spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/netty-all-4.1.17.Final.jar,spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/livy-core_2.12-0.7.1-incubating.jar,spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/livy-api-0.7.1-incubating.jar,spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/commons-codec-1.9.jar,spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/livy-rsc-0.7.1-incubating.jar,spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/livy-thriftserver-session-0.7.1-incubating.jar,spark://ip-100-85-112-151.us-west-2.compute.internal:41933/jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.yarn.submit.waitAppCompletion', 'false')\n",
      "('spark.yarn.secondary.jars', 'livy-api-0.7.1-incubating.jar,livy-rsc-0.7.1-incubating.jar,livy-thriftserver-session-0.7.1-incubating.jar,netty-all-4.1.17.Final.jar,commons-codec-1.9.jar,livy-core_2.12-0.7.1-incubating.jar,livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.pyspark.virtualenv.type', 'native')\n",
      "('spark.driver.host', 'ip-100-85-112-151.us-west-2.compute.internal')\n",
      "('spark.fileMetadataCache.enabled', 'false')\n",
      "('spark.yarn.dist.archives', 'file:/usr/lib/spark/R/lib/sparkr.zip#sparkr')\n",
      "('spark.pyspark.python', 'python3.7')\n",
      "('spark.yarn.maxAppAttempts', '1')\n",
      "('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'ip-100-85-112-151.us-west-2.compute.internal')\n",
      "('spark.sql.hive.metastore.sharedPrefixes', 'com.amazonaws.services.dynamodbv2')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true')\n",
      "('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true')\n",
      "('spark.livy.spark_major_version', '3')\n",
      "('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.repl.local.jars', 'file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.history.ui.port', '18080')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.resourceManager.cleanupExpiredHost', 'true')\n",
      "('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.yarn.historyServer.address', 'ip-100-85-112-151.us-west-2.compute.internal:18080')\n",
      "('spark.yarn.dist.files', 'file:/etc/spark/conf/hive-site.xml')\n",
      "('spark.executor.cores', '2')\n",
      "('spark.app.name', 'livy-session-0')\n",
      "('spark.files.fetchFailure.unRegisterOutputOnHost', 'true')\n",
      "('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://ip-100-85-112-151.us-west-2.compute.internal:20888/proxy/application_1670028769998_0002')\n",
      "('spark.master', 'yarn')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.app.startTime', '1670029805155')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.yarn.isPython', 'true')\n",
      "('spark.yarn.dist.jars', 'file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.driver.port', '41933')\n",
      "('spark.driver.appUIAddress', 'http://ip-100-85-112-151.us-west-2.compute.internal:4040')"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf68e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
