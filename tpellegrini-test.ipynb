{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a5fcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3.7', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv', 'spark.jars': 's3://tecton.ai.public/pip-repository/itorgation/tecton/0.2.10/tecton-udfs-spark-3.jar,s3://tecton.ai.public/jars/delta-core_2.12-1.0.1.jar', 'spark.sql.jsonGenerator.ignoreNullFields': 'false', 'spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs': 'false'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>8</td><td>application_1664065712648_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-10-23.us-west-2.compute.internal:20888/proxy/application_1664065712648_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-8-241.us-west-2.compute.internal:8042/node/containerlogs/container_1664065712648_0010_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>11</td><td>application_1664065712648_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-10-23.us-west-2.compute.internal:20888/proxy/application_1664065712648_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-11-81.us-west-2.compute.internal:8042/node/containerlogs/container_1664065712648_0013_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\":{\n",
    "    \"spark.pyspark.python\":\"python3.7\",\n",
    "    \"spark.pyspark.virtualenv.enabled\":\"true\",\n",
    "    \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "    \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\",\n",
    "    \"spark.jars\":\"s3://tecton.ai.public/pip-repository/itorgation/tecton/0.2.10/tecton-udfs-spark-3.jar,s3://tecton.ai.public/jars/delta-core_2.12-1.0.1.jar\",\n",
    "    \"spark.sql.jsonGenerator.ignoreNullFields\": \"false\",\n",
    "    \"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs\": \"false\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f76c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>13</td><td>application_1664065712648_0015</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-10-23.us-west-2.compute.internal:20888/proxy/application_1664065712648_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-11-81.us-west-2.compute.internal:8042/node/containerlogs/container_1664065712648_0015_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import FloatType\n",
    "import tecton\n",
    "\n",
    "MODEL_NAME = \"chat-yield\"\n",
    "BUCKET_NAME = \"gd-gdmlml-stage-sagemaker-us-west-2\"\n",
    "DATASET_VERSION = \"20220223\"\n",
    "DATASET_CHANNEL = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de784c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o90.parquet.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: BCYCASAXSAEAHEMG; S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==; Proxy: null), S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:246)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:209)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:833)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: BCYCASAXSAEAHEMG; S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==; Proxy: null), S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1862)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1415)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5378)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:970)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:108)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:135)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:411)\n",
      "\t... 23 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 458, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o90.parquet.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: BCYCASAXSAEAHEMG; S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==; Proxy: null), S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:246)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:209)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:833)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: BCYCASAXSAEAHEMG; S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==; Proxy: null), S3 Extended Request ID: yeRw1sAgnWvCoZTxxZoYxtjXm3dVlA+8m2ni+3bkadd+dPHad9unHUapSfPp/J/oss9vKMRJ83uD0yf74DLXnw==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1862)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1415)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1384)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1154)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:811)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:779)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:753)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:713)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:695)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:539)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5437)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5384)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5378)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:970)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:108)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:135)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:411)\n",
      "\t... 23 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load original chat-yield data, including model-specific features and shopper_ids + dates for Tecton lookup\n",
    "dataset_location = f\"s3://{BUCKET_NAME}/{MODEL_NAME}/data/raw/{MODEL_NAME}_{DATASET_VERSION}.parquet\"\n",
    "chat_yield_df = spark.read.parquet(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525b481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
